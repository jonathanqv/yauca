{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "56fa2b54-e21b-4f4a-8323-b412b17357cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,gsflow\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import gdal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f33b4fa1-78db-4a6a-b213-ab122d72a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "basinEPSG = {'Yauca':32718,\n",
    "            'Camana':32718,\n",
    "            'Ocona':32718,\n",
    "            'Quilca':32719,\n",
    "            'Tambo':32719}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "46863794-928b-4beb-b9d7-3dab665f129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = 'Yauca'\n",
    "\n",
    "yearStart = '2014'\n",
    "yearEnd = '2016'\n",
    "\n",
    "Path = '/mnt/c/Users/lrbk/Documents/CSM/Proyects/5_Watershed'\n",
    "metPath = os.path.join(Path,'4_MetDataProcessed','weather_Acholado_wIS',ws)\n",
    "flowsPath = os.path.join(Path,'4_MetDataProcessed','flows_SENAMHI',ws)\n",
    "demPath = os.path.join(Path,'2_GIS','1_Base',F'dem_{basinEPSG[ws]}.tif')\n",
    "outPath = '../data'\n",
    "inpPath = '../sim/base/input'\n",
    "obsPath = '../sim/base/obs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49586c-81f1-4438-a3ab-7107735a59d0",
   "metadata": {},
   "source": [
    "# 1. Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "727ada5e-8c57-4b83-b924-87960bed737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13149, 95)\n",
      "(13149, 93)\n"
     ]
    }
   ],
   "source": [
    "var = 'Prec'\n",
    "\n",
    "# Loading data and filling nans with -999 at stations and infrastructure\n",
    "dfPrec = pd.read_csv(os.path.join(metPath,F'{var}.csv'),index_col=0,parse_dates=True)\n",
    "dfPrec = dfPrec/25.4 # To inches\n",
    "si = dfPrec.columns[~dfPrec.columns.str.startswith('P_')] # selecting stations and infrastructure\n",
    "dfPrec[si] = dfPrec[si].fillna(-999)\n",
    "print(dfPrec.shape)\n",
    "\n",
    "#Droping pisco datasets with nan values, usually located in the shoreline or sea\n",
    "dfPrec = dfPrec.dropna(axis=1)\n",
    "print(dfPrec.shape)\n",
    "dfPrec= dfPrec.round(6)\n",
    "\n",
    "# Loading coordinates\n",
    "dfCPrec = pd.read_csv(os.path.join(metPath,F'coordinates{var}.csv'),index_col=0)\n",
    "dfCPrec = dfCPrec.loc[dfPrec.columns.values] # Filtering from df non nan values\n",
    "# geo processing\n",
    "dfCPrec['geometry'] = gpd.GeoSeries.from_wkt(dfCPrec['geometry'])\n",
    "dfCPrec = gpd.GeoDataFrame(dfCPrec, geometry='geometry',crs='EPSG:4326')\n",
    "dfCPrec = dfCPrec.to_crs(F'EPSG:{basinEPSG[ws]}')\n",
    "dfCPrec['x'] = dfCPrec.geometry.x\n",
    "dfCPrec['y'] = dfCPrec.geometry.y\n",
    "#dfC.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d39903-58ef-4af2-9b49-f79f41f4f388",
   "metadata": {},
   "source": [
    "# 2. TMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b35bb360-b71c-4549-a536-d0fe6957d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13149, 95)\n",
      "(13149, 94)\n"
     ]
    }
   ],
   "source": [
    "var = 'tmax'\n",
    "\n",
    "# Loading data and filling nans with -999 at stations and infrastructure\n",
    "dfTmax = pd.read_csv(os.path.join(metPath,F'{var}.csv'),index_col=0,parse_dates=True)\n",
    "dfTmax = dfTmax*9/5 + 32 # To F\n",
    "si = dfTmax.columns[~dfTmax.columns.str.startswith('P_')] # selecting sations and infrastructure\n",
    "dfTmax[si] = dfTmax[si].fillna(-999)\n",
    "print(dfTmax.shape)\n",
    "#Droping pisco datasets with nan values, usually located in the shoreline or sea\n",
    "dfTmax = dfTmax.dropna(axis=1)\n",
    "print(dfTmax.shape)\n",
    "dfTmax= dfTmax.round(6)\n",
    "\n",
    "# Loading coordinates\n",
    "dfCTmax = pd.read_csv(os.path.join(metPath,F'coordinates{var}.csv'),index_col=0)#,index_col=0,parse_dates=True\n",
    "dfCTmax = dfCTmax.loc[dfTmax.columns.values] # Filtering from df non nan values\n",
    "# geo processing\n",
    "dfCTmax['geometry'] = gpd.GeoSeries.from_wkt(dfCTmax['geometry'])\n",
    "dfCTmax = gpd.GeoDataFrame(dfCTmax, geometry='geometry',crs='EPSG:4326')\n",
    "dfCTmax = dfCTmax.to_crs(F'EPSG:{basinEPSG[ws]}')\n",
    "dfCTmax['x'] = dfCTmax.geometry.x\n",
    "dfCTmax['y'] = dfCTmax.geometry.y\n",
    "#dfC.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074ba72-e32e-4cd2-bbdf-befe37b77753",
   "metadata": {},
   "source": [
    "# 3. tmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7bd33b3-4b31-4dd6-90e6-c97c74144e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13149, 95)\n",
      "(13149, 94)\n"
     ]
    }
   ],
   "source": [
    "var = 'tmin'\n",
    "\n",
    "# Loading data and filling nans with -999 at stations and infrastructure\n",
    "dfTmin = pd.read_csv(os.path.join(metPath,F'{var}.csv'),index_col=0,parse_dates=True)\n",
    "dfTmin = dfTmin*9/5 + 32 # To F\n",
    "si = dfTmin.columns[~dfTmin.columns.str.startswith('P_')] # selecting sations and infrastructure\n",
    "dfTmin[si] = dfTmin[si].fillna(-999)\n",
    "print(dfTmin.shape)\n",
    "#Droping pisco datasets with nan values, usually located in the shoreline or sea\n",
    "dfTmin = dfTmin.dropna(axis=1)\n",
    "print(dfTmin.shape)\n",
    "# Rounding\n",
    "dfTmin= dfTmin.round(6)\n",
    "\n",
    "# Loading coordinates\n",
    "dfCTmin = pd.read_csv(os.path.join(metPath,F'coordinates{var}.csv'),index_col=0)#,index_col=0,parse_dates=True\n",
    "dfCTmin = dfCTmin.loc[dfTmin.columns.values] # Filtering from df non nan values\n",
    "# geo processing\n",
    "dfCTmin['geometry'] = gpd.GeoSeries.from_wkt(dfCTmin['geometry'])\n",
    "dfCTmin = gpd.GeoDataFrame(dfCTmin, geometry='geometry',crs='EPSG:4326')\n",
    "dfCTmin = dfCTmin.to_crs(F'EPSG:{basinEPSG[ws]}')\n",
    "dfCTmin['x'] = dfCTmin.geometry.x\n",
    "dfCTmin['y'] = dfCTmin.geometry.y\n",
    "#dfC.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f870becc-c623-4f0c-a7aa-0a8793b83cf5",
   "metadata": {},
   "source": [
    "# 4. Unifying dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bdcb669e-8672-49f8-952e-c2a58d9f43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTmax = dfTmax.loc[yearStart:yearEnd]\n",
    "dfTmin = dfTmin.loc[yearStart:yearEnd]\n",
    "dfPrec = dfPrec.loc[yearStart:yearEnd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2366f57c-fe2b-4816-bcba-9921c1d69ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTmax.columns = [F\"tmax_{ix}\" for ix,jx in enumerate(dfTmax.columns)]\n",
    "dfTmin.columns = [F\"tmin_{ix}\" for ix,jx in enumerate(dfTmin.columns)]\n",
    "dfPrec.columns = [F\"precip_{ix}\" for ix,jx in enumerate(dfPrec.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "851f4655-9b2e-452c-8e9c-42974b434f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = dfTmax.join(dfTmin).join(dfPrec)\n",
    "\n",
    "cols = merged.columns\n",
    "\n",
    "merged['year'] = merged.index.year\n",
    "merged['month'] = merged.index.month\n",
    "merged['day'] = merged.index.day\n",
    "merged['hour'] = merged.index.hour\n",
    "merged['minute'] = merged.index.minute\n",
    "merged['second'] = merged.index.second\n",
    "\n",
    "merged = merged[['year','month','day','hour','minute','second'] + cols.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e2ca0d-21d0-41d4-9fec-437d57e7837a",
   "metadata": {},
   "source": [
    "# 5. Adding Runoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b41e03-b7a0-40fa-ab38-b4fc98e901e6",
   "metadata": {},
   "source": [
    "## 5.1. Streamflow - not used yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "661d7928-f362-4b33-859b-dc6aa18fca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowFiles = [i for i in os.listdir(flowsPath)if int(i.split('_')[0]) in [1,2]]\n",
    "for i,j in enumerate(flowFiles):\n",
    "    name = j.split('_')[1].split('.')[0]\n",
    "    dfR = pd.read_csv(os.path.join(flowsPath,j),index_col=0,parse_dates=True)\n",
    "    dfR = dfR.loc[yearStart:yearEnd]\n",
    "    dfR = dfR.fillna(-999.0)\n",
    "    print(j,dfR.index.min(),dfR.index.max())\n",
    "    #merged[F'runoff_{i}'] = dfR\n",
    "    #dfR.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d70be-ed5c-4ddc-a679-13f67597a5e5",
   "metadata": {},
   "source": [
    "## 5.2. Streamflow - dummy zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8f29b9fc-c5e2-48ad-bc85-0b6df3fda610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy_df = pd.DataFrame(np.zeros(len(merged.index)),merged.index,columns=['runoff_0'])\n",
    "merged[F'runoff_{0}'] = np.zeros(len(merged.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea32042-de8c-4439-ab28-cc2902e2228d",
   "metadata": {},
   "source": [
    "## 5.3. reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e4f9bde-80cc-4ac5-9524-5817fe85d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only Condoroma first\n",
    "# resFiles = [i for i in os.listdir(reservFlowPath) if 'Pane' in i if 'Outflow' in i] # [-1] To choose Egasa \n",
    "# for i,j in enumerate(resFiles):\n",
    "#     name = j.split('_')[1].split('.')[0]\n",
    "#     dfR = pd.read_csv(os.path.join(reservFlowPath,j),index_col=0,parse_dates=True)\n",
    "#     dfR = dfR.loc[yearStart:yearEnd]\n",
    "#     print(name)\n",
    "#     #merged[F'runoff_{2+i}'] = dfR\n",
    "# #dfR.Flow_cms[dfR.Flow_cms.isnull()] # To check if nans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3413efa-0468-43a0-9fce-6eac837f8e8d",
   "metadata": {},
   "source": [
    "# 6. Reservoir gages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e1f9da3e-38fd-4687-942a-9f9a9d9d140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cotaPG = pd.read_csv(os.path.join(reservFlowPath,'Cota_PastoGrande_Moquegua.csv'),index_col=0,parse_dates=True)\n",
    "# cotaPG = cotaPG.loc[yearStart:yearEnd]\n",
    "# cotaPG = cotaPG-cotaPG.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b476e797-793c-409e-8abd-30a37905bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(F\"Starting level {cotaPG.iloc[0].item()}\")\n",
    "# cotaPG.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a6d212b8-69d4-4baa-94d0-899111e8bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlake = 1\n",
    "# for i in range(nlake):\n",
    "#     merged[F'gate_ht_{i}'] = np.zeros(len(merged.index))#*9999*100*2.54\n",
    "# for i in range(nlake):\n",
    "#     merged[F'lake_elev_{i}'] = cotaPG.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2600d435-5f68-42b8-88fe-f8b806286abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cotaPG.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b129c05-5ce7-48b4-b7a3-0500a2d90988",
   "metadata": {},
   "source": [
    "# 7. Exporting xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fc29ffec-736c-4bc4-9308-6638227bccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding elevation\n",
    "\n",
    "dataset = gdal.Open(demPath)\n",
    "band = dataset.GetRasterBand(1)\n",
    "cols = dataset.RasterXSize\n",
    "rows = dataset.RasterYSize\n",
    "transform = dataset.GetGeoTransform()\n",
    "xOrigin = transform[0]\n",
    "yOrigin = transform[3]\n",
    "pixelWidth = transform[1]\n",
    "pixelHeight = -transform[5]\n",
    "data = band.ReadAsArray()\n",
    "\n",
    "elevation = []\n",
    "for idx,coords in dfCPrec.iterrows():\n",
    "    col = int((coords.x - xOrigin) / pixelWidth)\n",
    "    row = int((yOrigin - coords.y ) / pixelHeight)\n",
    "    elevation.append(data[row][col])\n",
    "dfCPrec['Elevation'] = elevation\n",
    "\n",
    "elevation = []\n",
    "for idx,coords in dfCTmax.iterrows():\n",
    "    col = int((coords.x - xOrigin) / pixelWidth)\n",
    "    row = int((yOrigin - coords.y ) / pixelHeight)\n",
    "    elevation.append(data[row][col])\n",
    "dfCTmax['Elevation'] = elevation\n",
    "\n",
    "dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7ea28c56-5e83-47fb-b186-ddf0631fce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(outPath,'psta_elev.dat'),dfCPrec.Elevation.values)\n",
    "np.savetxt(os.path.join(outPath,'psta_x.dat'),dfCPrec.x.values)\n",
    "np.savetxt(os.path.join(outPath,'psta_y.dat'),dfCPrec.y.values)\n",
    "\n",
    "np.savetxt(os.path.join(outPath,'tsta_elev.dat'),dfCTmax.Elevation.values)\n",
    "np.savetxt(os.path.join(outPath,'tsta_x.dat'),dfCTmax.x.values)\n",
    "np.savetxt(os.path.join(outPath,'tsta_y.dat'),dfCTmax.y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb9700-5f94-4bfd-953a-f45f4ff41356",
   "metadata": {},
   "source": [
    "# 8. Getting nobs coordinates - Not used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5a9ac9e9-fbe1-4ad1-8cb5-0597aef9e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = gpd.read_file(F'../../../1_Outputs/{ws}/0_Auxiliar/{ws}.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0204624-8a63-4e02-9f98-08e90c9453da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flowSta = pd.read_csv('../../../4_MetDataProcessed/Notebooks/auxiliar/flowStations.csv')\n",
    "# flowSta['geometry'] = gpd.GeoSeries.from_wkt(flowSta.geometry)\n",
    "# flowSta = gpd.GeoDataFrame(flowSta)\n",
    "# flowSta = flowSta.set_crs(epsg=4326)\n",
    "# flowSta = flowSta.to_crs(epsg=basinEPSG[ws])\n",
    "\n",
    "# target = [int(i.split('_')[0]) for i in flowFiles]\n",
    "# target_g = flowSta[flowSta['NÂ°'].isin(target)]\n",
    "\n",
    "# tttt = gpd.sjoin(target_g, grid[['nHru', 'geometry']], how='left', predicate='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1aa944fe-fdbf-40a7-9585-b5d025f135a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tttt.to_csv(F'../../1_Outputs/{ws}/0_Auxiliar/nobs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e8bba-7841-4179-9302-0eecec5448e0",
   "metadata": {},
   "source": [
    "## 9.5 Climate By HRU - Run after creating .cbh files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "02315929-0d90-44ee-b986-97f609970d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "todel = ['tmax','tmin','precip']\n",
    "merged = merged[[i for i in merged.columns if i.split('_')[0] not in todel]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a3e45-a070-470e-9800-ccf80d40b6db",
   "metadata": {},
   "source": [
    "# 9. Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c56b0027-d092-4b5e-a8f0-9283e4461bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(os.path.join(inpPath,F'{ws}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6342c2cf-b8a1-4d51-8234-d85421749d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['Date'] = merged.index\n",
    "prms_data = gsflow.prms.PrmsData(data_df=merged,header='Tambo PRMS data from mixed sources')\n",
    "prms_data.write(os.path.join(inpPath,F'{ws.lower()}.data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba19644-9938-43cb-92ef-c83c72e09627",
   "metadata": {},
   "source": [
    "# 10. Transfer File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "70584b44-36e1-4276-992e-48a6dc042e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flowPath = '../../../../4_MetDataProcessed/flows_Infrastructure'\n",
    "# flowMoquegua = pd.read_csv(os.path.join(flowPath,'Outflows_PastoGrande_Moquegua.csv'),index_col=0,parse_dates=True)\n",
    "# flowTambo = pd.read_csv(os.path.join(flowPath,'Outflows_PastoGrande_Tambo.csv'),index_col=0,parse_dates=True)\n",
    "\n",
    "# flowMoquegua.columns = ['Flow_Moquegua_cms']\n",
    "# flowTambo.columns = ['Flow_Tambo_cms']\n",
    "\n",
    "# flows = pd.concat([flowTambo,flowMoquegua],axis=1)\n",
    "# flows = flows / 0.3048**3\n",
    "# flows.columns = ['Flow_Tambo_cfs','Flow_Moquegua_cfs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "141d327e-dbea-4456-a7f5-c5fcde35c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Water Use - PastoGrande\n",
    "# cols = ['year','month','day','src_id','dest_type','dest_id','transfer_rate']\n",
    "# transfer = pd.DataFrame(columns=cols)\n",
    "# idx = 0\n",
    "# for i in pd.date_range(F'{yearStart}-01-01',F'{yearEnd}-12-31'):\n",
    "#     for j in flows.columns:\n",
    "#         idx+=1\n",
    "#         transfer.loc[idx,'year'] = i.year\n",
    "#         transfer.loc[idx,'month'] = i.month\n",
    "#         transfer.loc[idx,'day'] = i.day\n",
    "#         transfer.loc[idx,'src_id'] = 1 # lake #1\n",
    "#         if 'Tambo' in j:\n",
    "#             transfer.loc[idx,'dest_type'] = 1 # segment\n",
    "#             transfer.loc[idx,'dest_id'] = 6\n",
    "#             transfer.loc[idx,'transfer_rate'] = F\"{flows.loc[i,j]:.5f}\"\n",
    "#         elif 'Moquegua' in j:\n",
    "#             transfer.loc[idx,'dest_type'] = 4 # external\n",
    "#             transfer.loc[idx,'dest_id'] = 1\n",
    "#         transfer.loc[idx,'transfer_rate'] = F\"{flows.loc[i,j]:.5f}\"\n",
    "\n",
    "\n",
    "# # flows = flows[['Flow_Tambo_cfs']]\n",
    "# # idx=0\n",
    "# # for i in pd.date_range(F'{yearStart}-01-01',F'{yearEnd}-12-31'):\n",
    "# #     idx+=1\n",
    "# #     transfer.loc[idx,'year'] = i.year\n",
    "# #     transfer.loc[idx,'month'] = i.month\n",
    "# #     transfer.loc[idx,'day'] = i.day\n",
    "# #     transfer.loc[idx,'src_id'] = 1 # lake #1\n",
    "# #     transfer.loc[idx,'dest_type'] = 1 # externatl\n",
    "# #     transfer.loc[idx,'dest_id'] = 6\n",
    "# #     transfer.loc[idx,'transfer_rate'] = F\"{(flows.loc[i]).item():.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "42175540-306c-404a-8d61-510378f3757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(os.path.join(oPath,\"external.transfer\"), \"w\")\n",
    "# f.write(\"Trying transfer file \\n\")\n",
    "# f.write('\\t'.join(transfer.columns))\n",
    "# f.write(\"\\n#### \\n\")\n",
    "# dfAsString = transfer.to_string(header=False, index=False)\n",
    "# f.write(dfAsString)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700aaa5-2629-4383-bc47-b58e5091cff0",
   "metadata": {},
   "source": [
    "# Formatting observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c7b2072-4e34-4aee-a155-f4a43c0cb223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['41_Yauca.csv']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(flowsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "60818eca-b335-4aa7-a9e8-3b0d97326374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Flows\n",
    "# SantaRosaPath = os.path.join(flowsPath,'34_PuenteSantaRosa.csv')\n",
    "# PascanaPath = os.path.join(flowsPath,'35_LaPascana.csv')\n",
    "\n",
    "# SantaRosa = pd.read_csv(SantaRosaPath,index_col=0,parse_dates=True)\n",
    "# SantaRosa = SantaRosa.loc[yearStart:yearEnd] /0.3048**3\n",
    "# SantaRosa = SantaRosa.fillna(-999)\n",
    "# SantaRosa.to_csv(os.path.join(obsPath,'Flow_SantaRosa.csv'))\n",
    "\n",
    "# Pascana = pd.read_csv(PascanaPath,index_col=0,parse_dates=True)\n",
    "# Pascana= Pascana.loc[yearStart:yearEnd] /0.3048**3\n",
    "# Pascana = Pascana.fillna(-999)\n",
    "# Pascana.to_csv(os.path.join(obsPath,'Flow_Pascana.csv'))\n",
    "\n",
    "dates = pd.date_range(start=F'{yearStart}-01-01',end=F'{yearEnd}-12-31')\n",
    "yauca = pd.read_csv(os.path.join(flowsPath,'41_Yauca.csv'),index_col=0,parse_dates=True)\n",
    "yauca= yauca.loc[yearStart:yearEnd] /0.3048**3\n",
    "yauca = yauca.fillna(-999)\n",
    "for i in dates:\n",
    "    try:\n",
    "        isinstance(yauca.loc[i],float)\n",
    "    except:\n",
    "        yauca.loc[i] = -999\n",
    "yauca.to_csv(os.path.join(obsPath,'flow_yauca.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "997a7e12-a093-4bf8-a5e1-59a2dbc35d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reservoirs\n",
    "# PastoGrandePath = os.path.join(reservFlowPath,'Volume_PastoGrande_Moquegua.csv')\n",
    "# PastoGrande = pd.read_csv(PastoGrandePath,index_col=0,parse_dates=True)\n",
    "# PastoGrande = PastoGrande.loc[yearStart:yearEnd]\n",
    "# PastoGrande['Volume_acr-ft'] = PastoGrande*1E6/4046.86/0.3048\n",
    "# PastoGrande = PastoGrande.fillna(-999)\n",
    "# PastoGrande['Volume_acr-ft'].to_csv(os.path.join(obsPath,'Volume_PastoGrande.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
